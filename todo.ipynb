{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.042416666666664\n",
      "192509\n"
     ]
    }
   ],
   "source": [
    "N = len(data)\n",
    "sc = 5*N/1000\n",
    "mins = sc/60\n",
    "print(mins)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-61ff364f08e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 14"
     ]
    }
   ],
   "source": [
    "dir(data)\n",
    "data[\"title\"][0]\n",
    "data[\"abstract\"][14]\n",
    "i = 0\n",
    "for a in data[\"abstract\"]:\n",
    "    if type(a).__name__==\"float\":\n",
    "        i += 1\n",
    "        #print(a, \"!\", i)\n",
    "print(\"{}/{} articles don't have abstract: Corresponds to {}%\".format(i, len(data), i/len(data)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1- Tokenization/stemming\n",
    "2- Normalize\n",
    "3- Build vocabulary after stemming\n",
    "4- Collect TF-IDF\n",
    "5- Implement the cosine similarity system mentioned in lectures\n",
    "\n",
    "\n",
    "Exception for 1 token input\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[\"abstract\"][2]\n",
    "words= [word for word in word_tokenize(text) if word.isalnum()]\n",
    "N = len(words)\n",
    "tf = np.zeros((N,))\n",
    "idf = np.zeros((N,))\n",
    "counter = Counter(words)\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(word) for word in words]\n",
    "lanc = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stems2 = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "#print(stems2)\n",
    "#print(\"-\"*40)\n",
    "#print(stems)\n",
    "#print(\"-\"*40)\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import spatial\n",
    "import scipy\n",
    "from sortedcontainers import SortedList\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, tokens, word2id=None, doc_id=None, \n",
    "                 cord_uid=None, abstract=None, title=None):\n",
    "        self.tokens = tokens\n",
    "        self.doc_id = doc_id\n",
    "        self.cord_uid = cord_uid\n",
    "        self.abstract = abstract\n",
    "        self.title = title\n",
    "        self.tf_dict = Counter(tokens)\n",
    "        if word2id:\n",
    "            self.tf_vec = self.get_tf_vector_(word2id)\n",
    "            \n",
    "    def get_tf_vector_(self, word2id):\n",
    "        tokens = self.tokens\n",
    "        V = len(word2id) + 1\n",
    "        tf_vec = [0]*V\n",
    "        for token in tokens:\n",
    "            if token in word2id:\n",
    "                tf_vec[word2id[token]] += 1\n",
    "        # store as sparse matrix\n",
    "        tf_vec = scipy.sparse.csr_matrix(tf_vec)\n",
    "        tf_vec[tf_vec>0] = np.log10(tf_vec[tf_vec>0]) + 1\n",
    "        return tf_vec\n",
    "\n",
    "class TrecIR:\n",
    "    def __init__(self, corpus, process_type=\"stem\"):\n",
    "        self.corpus = corpus\n",
    "        self.process_type = process_type\n",
    "        if process_type == \"stem\":\n",
    "            self.porter = PorterStemmer()\n",
    "            self.proc_token = lambda x: self.porter.stem(x)\n",
    "        elif process_type == \"lemmatize\":\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.proc_token = lambda x: self.lemmatizer.lemmatize(x)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        self.process_corpus_()\n",
    "        \n",
    "  \n",
    "    def get_ranked_docs(self, query):\n",
    "        # todo: stopword elim to speed up\n",
    "        query_doc = self.process_text_(query)\n",
    "        \n",
    "        # idf: shape: (V, 1)\n",
    "        # doc_tf: (1, V)\n",
    "        # q_tf_idf: (V, 1)\n",
    "        q_tf_idf = self.idf.multiply(query_doc.tf_vec.T)\n",
    "        ranked_docs = SortedList()\n",
    "        doc_ids = set.union(*[self.posting_list[word] \n",
    "                                  for word in query_doc.tokens])\n",
    "        for doc_id in doc_ids:\n",
    "            doc = self.docs[doc_id]\n",
    "            doc_tf_idf = self.tf_idf[doc_id]\n",
    "            sim = cosine_similarity(q_tf_idf.T, doc_tf_idf)\n",
    "            ranked_docs.add((-sim, doc))\n",
    "        \n",
    "        return [tup[1] for tup in ranked_docs]\n",
    "    \n",
    "    def tokenize_(self, text):\n",
    "        return [word for word in word_tokenize(text) \n",
    "                      if word.isalnum()]\n",
    "    \n",
    "    def process_(self, tokens):\n",
    "        return [self.proc_token(t) for t in tokens]\n",
    "        \n",
    "    def process_text_(self, text):\n",
    "        tokenized_text = self.tokenize_(text)\n",
    "        tokenized_text = self.process_(tokenized_text)\n",
    "        return Document(tokenized_text, self.word2id)\n",
    "        \n",
    "    def process_corpus_(self):\n",
    "        t1 = time.time()\n",
    "        self.docs = []\n",
    "        # 1 for UNK\n",
    "        self.idf = dict({0: 1})\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.cord2id = dict()\n",
    "        self.id2cord = dict()\n",
    "        self.posting_list = dict()\n",
    "        # posting_list: key: string, value: set\n",
    "        word_index = 1\n",
    "        doc_index = 0\n",
    "        for i in range(len(self.corpus)):\n",
    "            if i % 10_000 == 1:\n",
    "                dt = time.time()-t1\n",
    "                t1 = time.time()\n",
    "                print(\"{}%, time: {}\".format(i/len(self.corpus)*100, dt), flush=True)\n",
    "                \n",
    "            cord_uid = data[\"cord_uid\"][0]\n",
    "            title = self.corpus[\"title\"][i]\n",
    "            abstract = self.corpus[\"abstract\"][i]\n",
    "            if type(title).__name__==\"float\" or type(abstract).__name__ == \"float\":\n",
    "                # ABSTRACT DOES NOT EXIST: SKIP FOR NOW\n",
    "                continue\n",
    "            text= title + \" \" + abstract\n",
    "            tokenized_text = self.tokenize_(text)\n",
    "            tokenized_text = self.process_(tokenized_text)\n",
    "            for word in tokenized_text:\n",
    "                # add word to dictionary\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = word_index\n",
    "                    self.id2word[word_index] = word\n",
    "                    word_index += 1\n",
    "                # add doc to posting_list\n",
    "                if word not in self.posting_list:\n",
    "                    self.posting_list[word] = set()\n",
    "                self.posting_list[word].add(doc_index)\n",
    "                \n",
    "            if cord_uid not in self.cord2id:\n",
    "                self.cord2id[cord_uid] = doc_index\n",
    "                self.id2cord[doc_index] = cord_uid\n",
    "                doc_index += 1\n",
    "                \n",
    "            doc = Document(tokenized_text, word2id=None, \n",
    "                           doc_id=self.cord2id[cord_uid], \n",
    "                           cord_uid=cord_uid, title=title, \n",
    "                           abstract=abstract)\n",
    "            self.docs.append(doc)\n",
    "            for word, tf in doc.tf_dict.items():\n",
    "                index = self.word2id[word]\n",
    "                if index not in self.idf: \n",
    "                    self.idf[index] = 0\n",
    "                self.idf[index] += 1\n",
    "        \n",
    "        # idf: (V, 1);\n",
    "        for doc in self.docs:\n",
    "            doc.tf_vec = doc.get_tf_vector_(self.word2id)\n",
    "        N = len(self.corpus)\n",
    "        self.idf = np.array(list(zip(*sorted(self.idf.items())))[1])\n",
    "        self.idf = scipy.sparse.csr_matrix(np.log10(N/self.idf)).T\n",
    "        # tf: (N, V)\n",
    "        self.doc_tf = scipy.sparse.vstack([doc.tf_vec for doc in self.docs])\n",
    "        # tf_idf: (N, V)\n",
    "        self.tf_idf = (self.idf.multiply(self.doc_tf.T)).T\n",
    "        \n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1%, time: 0.00896596908569336\n",
      "ug7v899j\n"
     ]
    }
   ],
   "source": [
    "trec_ir = TrecIR(data)\n",
    "query = \"a question...\" # this if for test\n",
    "#qeuries = get_queries() # TODO: use the topic doc to get queries\n",
    "docs = trec_ir.get_ranked_docs(query) #TODO\n",
    "for doc in docs:\n",
    "    print(doc.cord_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9718"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trec_ir.posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ug7v899j\n",
      "Nitric oxide: a pro-inflammatory mediator in lung disease?\n",
      "1001\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"climates\" # this if for test#qeuries = get_queries() # TODO: use the topic doc to get queries\n",
    "docs = trec_ir.get_ranked_docs(query) #TODO\n",
    "for doc in docs:\n",
    "    print(doc.cord_uid)\n",
    "    print(doc.title)\n",
    "    print(doc.abstract)\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
