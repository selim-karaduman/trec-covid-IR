{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54865/192509 articles don't have abstract: Corresponds to 28.499966235344843%\n"
     ]
    }
   ],
   "source": [
    "dir(data)\n",
    "data[\"title\"][0]\n",
    "data[\"abstract\"][14]\n",
    "i = 0\n",
    "for a in data[\"abstract\"]:\n",
    "    if type(a).__name__==\"float\":\n",
    "        i += 1\n",
    "        #print(a, \"!\", i)\n",
    "print(\"{}/{} articles don't have abstract: Corresponds to {}%\".format(i, len(data), i/len(data)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1- Tokenization/stemming\n",
    "2- Normalize\n",
    "3- Build vocabulary after stemming\n",
    "4- Collect TF-IDF\n",
    "5- Implement the cosine similarity system mentioned in lectures\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[\"abstract\"][2]\n",
    "words= [word for word in word_tokenize(text) if word.isalnum()]\n",
    "N = len(words)\n",
    "tf = np.zeros((N,))\n",
    "idf = np.zeros((N,))\n",
    "counter = Counter(words)\n",
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(word) for word in words]\n",
    "lanc = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stems2 = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "#print(stems2)\n",
    "#print(\"-\"*40)\n",
    "#print(stems)\n",
    "#print(\"-\"*40)\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import spatial\n",
    "import scipy\n",
    "from sortedcontainers import SortedList\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, tokens, word2id=None, doc_id=None, \n",
    "                 cord_uid=None, abstract=None, title=None):\n",
    "        self.tokens = tokens\n",
    "        self.doc_id = doc_id\n",
    "        self.cord_uid = cord_uid\n",
    "        self.abstract = abstract\n",
    "        self.title = title\n",
    "        self.tf_dict = Counter(tokens)\n",
    "        if word2id:\n",
    "            self.tf_vec = self.get_tf_vector_(word2id)\n",
    "            \n",
    "    def get_tf_vector_(self, word2id):\n",
    "        tokens = self.tokens\n",
    "        V = len(word2id) + 1\n",
    "        tf_vec = [0]*V\n",
    "        for token in tokens:\n",
    "            if token in word2id:\n",
    "                tf_vec[word2id[token]] += 1\n",
    "        # store as sparse matrix\n",
    "        tf_vec = scipy.sparse.csr_matrix(tf_vec)\n",
    "        tf_vec[tf_vec>0] = np.log10(tf_vec[tf_vec>0]) + 1\n",
    "        return tf_vec\n",
    "\n",
    "class TrecIR:\n",
    "    def __init__(self, corpus, process_type=\"stem\"):\n",
    "        self.corpus = corpus\n",
    "        self.process_type = process_type\n",
    "        if process_type == \"stem\":\n",
    "            self.porter = PorterStemmer()\n",
    "            self.proc_token = lambda x: self.porter.stem(x)\n",
    "        elif process_type == \"lemmatize\":\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.proc_token = lambda x: self.lemmatizer.lemmatize(x)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        self.process_corpus_()\n",
    "        \n",
    "  \n",
    "    def get_ranked_docs(self, query):\n",
    "        # todo: stopword elim to speed up\n",
    "        query_doc = self.process_text_(query)\n",
    "        \n",
    "        # idf: shape: (V, 1)\n",
    "        # doc_tf: (1, V)\n",
    "        # q_tf_idf: (V, 1)\n",
    "        q_tf_idf = self.idf.multiply(query_doc.tf_vec.T)\n",
    "        ranked_docs = SortedList()\n",
    "        doc_ids = set.union(*[self.posting_list[word] \n",
    "                                  for word in query_doc.tokens])\n",
    "        for doc_id in doc_ids:\n",
    "            doc = self.docs[doc_id]\n",
    "            doc_tf_idf = self.tf_idf[doc_id]\n",
    "            sim = cosine_similarity(q_tf_idf.T, doc_tf_idf)\n",
    "            ranked_docs.add((-sim, doc))\n",
    "        \n",
    "        return [tup[1] for tup in ranked_docs]\n",
    "    \n",
    "    def tokenize_(self, text):\n",
    "        return [word for word in word_tokenize(text) \n",
    "                      if word.isalnum()]\n",
    "    \n",
    "    def process_(self, tokens):\n",
    "        return [self.proc_token(t) for t in tokens]\n",
    "        \n",
    "    def process_text_(self, text):\n",
    "        tokenized_text = self.tokenize_(text)\n",
    "        tokenized_text = self.process_(tokenized_text)\n",
    "        return Document(tokenized_text, self.word2id)\n",
    "        \n",
    "    def process_corpus_(self):\n",
    "        self.docs = []\n",
    "        # 1 for UNK\n",
    "        self.idf = dict({0: 1})\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.cord2id = dict()\n",
    "        self.id2cord = dict()\n",
    "        self.posting_list = dict()\n",
    "        # posting_list: key: string, value: set\n",
    "        word_index = 1\n",
    "        doc_index = 0\n",
    "        for i in range(len(self.corpus)):\n",
    "            if i % 10 == 1:\n",
    "                print(\"{}%\".format(i/len(self.corpus)*100), flush=True)\n",
    "                \n",
    "            cord_uid = data[\"cord_uid\"][0]\n",
    "            title = self.corpus[\"title\"][i]\n",
    "            abstract = self.corpus[\"abstract\"][i]\n",
    "            if type(title).__name__==\"float\" or type(abstract).__name__ == \"float\":\n",
    "                # ABSTRACT DOES NOT EXIST: SKIP FOR NOW\n",
    "                continue\n",
    "            text= title + \" \" + abstract\n",
    "            if cord_uid not in self.cord2id:\n",
    "                self.cord2id[cord_uid] = doc_index\n",
    "                self.id2cord[doc_index] = cord_uid\n",
    "                doc_index += 1\n",
    "            \n",
    "            tokenized_text = self.tokenize_(text)\n",
    "            tokenized_text = self.process_(tokenized_text)\n",
    "            for word in tokenized_text:\n",
    "                # add word to dictionary\n",
    "                if word not in self.word2id:\n",
    "                    self.word2id[word] = word_index\n",
    "                    self.id2word[word_index] = word\n",
    "                    word_index += 1\n",
    "                # add doc to posting_list\n",
    "                if word not in self.posting_list:\n",
    "                    self.posting_list[word] = set()\n",
    "                self.posting_list[word].add(doc_index)\n",
    "            doc = Document(tokenized_text, word2id=None, \n",
    "                           doc_id=self.cord2id[cord_uid], \n",
    "                           cord_uid=cord_uid, title=title, \n",
    "                           abstract=abstract)\n",
    "            self.docs.append(doc)\n",
    "            for word, tf in doc.tf_dict.items():\n",
    "                index = self.word2id[word]\n",
    "                if index not in self.idf: \n",
    "                    self.idf[index] = 0\n",
    "                self.idf[index] += 1\n",
    "        \n",
    "        # idf: (V, 1);\n",
    "        for doc in self.docs:\n",
    "            doc.tf_vec = doc.get_tf_vector_(self.word2id)\n",
    "        N = len(self.corpus)\n",
    "        self.idf = np.array(list(zip(*sorted(self.idf.items())))[1])\n",
    "        self.idf = scipy.sparse.csr_matrix(np.log10(N/self.idf)).T\n",
    "        # tf: (N, V)\n",
    "        self.doc_tf = scipy.sparse.vstack([doc.tf_vec for doc in self.docs])\n",
    "        # tf_idf: (N, V)\n",
    "        self.tf_idf = (self.idf.multiply(self.doc_tf.T)).T\n",
    "        \n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0%\n",
      "ug7v899j\n"
     ]
    }
   ],
   "source": [
    "trec_ir = TrecIR(data)\n",
    "query = \"a question...\" # this if for test\n",
    "#qeuries = get_queries() # TODO: use the topic doc to get queries\n",
    "docs = trec_ir.get_ranked_docs(query) #TODO\n",
    "for doc in docs:\n",
    "    print(doc.cord_uid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
